#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´QAæµç¨‹æµ‹è¯•è„šæœ¬
æµ‹è¯•å¤šæŸ¥è¯¢ç”Ÿæˆâ†’æ··åˆæ£€ç´¢â†’QAçš„å®Œæ•´æµç¨‹
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.qa import ConversationChain, Memory, PromptTemplate
from modules.retrieval.vector_store import VectorStore
from modules.retrieval.bm25_retriever import BM25Retriever
from modules.retrieval.hybrid_retriever import HybridRetriever
from config.settings import settings


def create_test_documents():
    """åˆ›å»ºæµ‹è¯•æ–‡æ¡£"""
    return [
        {
            'text': 'äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£å’Œæ„å»ºæ™ºèƒ½ä½“ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿæ„ŸçŸ¥ç¯å¢ƒå¹¶é‡‡å–è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–å…¶æˆåŠŸçš„æœºä¼šã€‚',
            'start': 0.0,
            'end': 10.0,
            'confidence': 0.95
        },
        {
            'text': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚',
            'start': 10.0,
            'end': 20.0,
            'confidence': 0.93
        },
        {
            'text': 'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚',
            'start': 20.0,
            'end': 30.0,
            'confidence': 0.91
        },
        {
            'text': 'è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„é‡è¦åº”ç”¨é¢†åŸŸï¼Œè‡´åŠ›äºè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚',
            'start': 30.0,
            'end': 40.0,
            'confidence': 0.89
        },
        {
            'text': 'è®¡ç®—æœºè§†è§‰æ˜¯å¦ä¸€ä¸ªé‡è¦çš„AIåº”ç”¨ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºèƒ½å¤Ÿä»å›¾åƒæˆ–è§†é¢‘ä¸­è·å–ã€å¤„ç†å’Œç†è§£è§†è§‰ä¿¡æ¯ã€‚',
            'start': 40.0,
            'end': 50.0,
            'confidence': 0.87
        },
        {
            'text': 'å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æ¸¸æˆã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚',
            'start': 50.0,
            'end': 60.0,
            'confidence': 0.85
        },
        {
            'text': 'AIåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨åŒ…æ‹¬ç–¾ç—…è¯Šæ–­ã€è¯ç‰©å‘ç°ã€ä¸ªæ€§åŒ–æ²»ç–—ç­‰ï¼Œå¤§å¤§æé«˜äº†åŒ»ç–—æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚',
            'start': 60.0,
            'end': 70.0,
            'confidence': 0.88
        },
        {
            'text': 'è‡ªåŠ¨é©¾é©¶æ±½è½¦æ˜¯AIæŠ€æœ¯çš„é‡è¦åº”ç”¨ä¹‹ä¸€ï¼Œé€šè¿‡ä¼ æ„Ÿå™¨å’Œç®—æ³•å®ç°è½¦è¾†çš„è‡ªä¸»å¯¼èˆªå’Œé©¾é©¶å†³ç­–ã€‚',
            'start': 70.0,
            'end': 80.0,
            'confidence': 0.86
        },
        {
            'text': 'é‡‘èé£æ§åˆ©ç”¨AIæŠ€æœ¯åˆ†æå¤§é‡äº¤æ˜“æ•°æ®ï¼Œè¯†åˆ«æ¬ºè¯ˆè¡Œä¸ºï¼Œè¯„ä¼°ä¿¡ç”¨é£é™©ï¼Œæé«˜é‡‘èç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚',
            'start': 80.0,
            'end': 90.0,
            'confidence': 0.84
        },
        {
            'text': 'æ™ºèƒ½å®¢æœç³»ç»Ÿä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œèƒ½å¤Ÿç†è§£ç”¨æˆ·é—®é¢˜å¹¶æä¾›å‡†ç¡®çš„å›ç­”ï¼Œå¤§å¤§æé«˜äº†å®¢æˆ·æœåŠ¡æ•ˆç‡ã€‚',
            'start': 90.0,
            'end': 100.0,
            'confidence': 0.82
        }
    ]


def setup_retrievers():
    """è®¾ç½®æ£€ç´¢å™¨"""
    print("è®¾ç½®æ£€ç´¢å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    documents = create_test_documents()
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = VectorStore()
    vector_store.add_documents(documents, text_field="text")
    
    # åˆ›å»ºBM25æ£€ç´¢å™¨
    bm25_retriever = BM25Retriever(language='auto')
    bm25_retriever.add_documents(documents, text_field="text")
    
    # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
    hybrid_retriever = HybridRetriever(
        vector_store=vector_store,
        bm25_retriever=bm25_retriever,
        vector_weight=0.6,
        bm25_weight=0.4,
        fusion_method="weighted_average"
    )
    
    print(f"âœ“ æ£€ç´¢å™¨è®¾ç½®å®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
    return hybrid_retriever


def test_multi_query_generation():
    """æµ‹è¯•å¤šæŸ¥è¯¢ç”Ÿæˆ"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å¤šæŸ¥è¯¢ç”Ÿæˆ")
    print("=" * 50)
    
    try:
        from modules.retrieval.multi_query import MultiQueryGenerator
        
        # åˆ›å»ºå¤šæŸ¥è¯¢ç”Ÿæˆå™¨
        models_dir = settings.PROJECT_ROOT / "models"
        generator = MultiQueryGenerator(cache_dir=str(models_dir))
        
        # æµ‹è¯•æŸ¥è¯¢
        test_query = "äººå·¥æ™ºèƒ½æœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ"
        print(f"åŸå§‹æŸ¥è¯¢: {test_query}")
        
        # ç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        result = generator.generate_queries(test_query)
        
        print(f"\nç”Ÿæˆäº† {len(result.generated_queries)} ä¸ªæ‰©å±•æŸ¥è¯¢:")
        for i, generated_query in enumerate(result.generated_queries):
            print(f"{i+1}. {generated_query.query} [æƒé‡: {generated_query.weight:.3f}]")
        
        print("\nâœ… å¤šæŸ¥è¯¢ç”Ÿæˆæµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å¤šæŸ¥è¯¢ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_hybrid_retrieval():
    """æµ‹è¯•æ··åˆæ£€ç´¢"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ··åˆæ£€ç´¢")
    print("=" * 50)
    
    try:
        # è®¾ç½®æ£€ç´¢å™¨
        hybrid_retriever = setup_retrievers()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "AIåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
            "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»",
            "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨"
        ]
        
        for query in test_queries:
            print(f"\næŸ¥è¯¢: {query}")
            
            # æ‰§è¡Œæ··åˆæ£€ç´¢
            results = hybrid_retriever.search(query, top_k=3)
            
            print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç»“æœ:")
            for i, result in enumerate(results):
                print(f"{i+1}. [åˆ†æ•°: {result['score']:.3f}] {result['text']}")
                print(f"   æ—¶é—´: {result['start']:.1f}s - {result['end']:.1f}s")
        
        print("\nâœ… æ··åˆæ£€ç´¢æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ··åˆæ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_complete_qa_flow():
    """æµ‹è¯•å®Œæ•´QAæµç¨‹"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæ•´QAæµç¨‹")
    print("=" * 50)
    
    try:
        # è®¾ç½®æ£€ç´¢å™¨
        hybrid_retriever = setup_retrievers()
        
        # åˆ›å»ºå¯¹è¯é“¾
        conversation_chain = ConversationChain(retriever=hybrid_retriever)
        
        # æµ‹è¯•å¤šè½®å¯¹è¯
        test_conversations = [
            "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ",
            "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "AIåœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›å…·ä½“åº”ç”¨ï¼Ÿ",
            "åˆšæ‰æåˆ°çš„åº”ç”¨ä¸­ï¼Œå“ªä¸ªæœ€æœ‰å‰æ™¯ï¼Ÿ",
            "ä½ èƒ½æ€»ç»“ä¸€ä¸‹AIæŠ€æœ¯çš„å‘å±•è¶‹åŠ¿å—ï¼Ÿ"
        ]
        
        for i, question in enumerate(test_conversations):
            print(f"\nç¬¬{i+1}è½®å¯¹è¯:")
            print(f"ç”¨æˆ·: {question}")
            
            # æ‰§è¡Œå®Œæ•´çš„QAæµç¨‹
            result = conversation_chain.chat(question, top_k=5)
            
            print(f"AI: {result['response']}")
            print(f"æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(result['retrieved_docs'])}")
            print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(result['context'])}")
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£
            if result['retrieved_docs']:
                print("ç›¸å…³æ–‡æ¡£:")
                for j, doc in enumerate(result['retrieved_docs'][:3]):
                    print(f"  {j+1}. [ç›¸ä¼¼åº¦: {doc.get('similarity', 0):.3f}] {doc.get('text', doc.get('document', {}).get('text', ''))[:50]}...")
        
        print("\nâœ… å®Œæ•´QAæµç¨‹æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å®Œæ•´QAæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_performance_metrics():
    """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€§èƒ½æŒ‡æ ‡")
    print("=" * 50)
    
    try:
        import time
        
        # è®¾ç½®æ£€ç´¢å™¨
        hybrid_retriever = setup_retrievers()
        
        # åˆ›å»ºå¯¹è¯é“¾
        conversation_chain = ConversationChain(retriever=hybrid_retriever)
        
        # æµ‹è¯•æŸ¥è¯¢
        test_query = "äººå·¥æ™ºèƒ½çš„ä¸»è¦åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ"
        
        # æµ‹è¯•å„ç¯èŠ‚è€—æ—¶
        start_time = time.time()
        
        # 1. å¤šæŸ¥è¯¢ç”Ÿæˆ
        multi_query_start = time.time()
        multi_query_result = conversation_chain.multi_query.generate_queries(test_query)
        multi_query_time = time.time() - multi_query_start
        
        # 2. æ£€ç´¢
        retrieval_start = time.time()
        retrieved_docs = conversation_chain._retrieve_documents(test_query, top_k=5)
        retrieval_time = time.time() - retrieval_start
        
        # 3. ä¸Šä¸‹æ–‡æ„å»º
        context_start = time.time()
        context = conversation_chain._build_context(retrieved_docs, test_query)
        context_time = time.time() - context_start
        
        # 4. LLMç”Ÿæˆ
        llm_start = time.time()
        response = conversation_chain._call_openai(test_query, context)
        llm_time = time.time() - llm_start
        
        total_time = time.time() - start_time
        
        # è¾“å‡ºæ€§èƒ½æŒ‡æ ‡
        print(f"æŸ¥è¯¢: {test_query}")
        print(f"\næ€§èƒ½æŒ‡æ ‡:")
        print(f"å¤šæŸ¥è¯¢ç”Ÿæˆ: {multi_query_time:.3f}s ({len(multi_query_result.generated_queries)} ä¸ªæŸ¥è¯¢)")
        print(f"æ–‡æ¡£æ£€ç´¢: {retrieval_time:.3f}s ({len(retrieved_docs)} ä¸ªæ–‡æ¡£)")
        print(f"ä¸Šä¸‹æ–‡æ„å»º: {context_time:.3f}s ({len(context)} å­—ç¬¦)")
        print(f"LLMç”Ÿæˆ: {llm_time:.3f}s ({len(response)} å­—ç¬¦)")
        print(f"æ€»è€—æ—¶: {total_time:.3f}s")
        
        print("\nâœ… æ€§èƒ½æŒ‡æ ‡æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æŒ‡æ ‡æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹å®Œæ•´QAæµç¨‹æµ‹è¯•...")
    print("=" * 50)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("é…ç½®ä¿¡æ¯:")
    print(f"LLMæä¾›å•†: {settings.get_model_config('llm', 'provider')}")
    print(f"æ¨¡å‹åç§°: {settings.get_model_config('llm', 'openai', 'model_name')}")
    print(f"èåˆæ–¹æ³•: weighted_average")
    print(f"å‘é‡æƒé‡: 0.6, BM25æƒé‡: 0.4")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_multi_query_generation,
        test_hybrid_retrieval,
        test_complete_qa_flow,
        test_performance_metrics
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®Œæ•´QAæµç¨‹å®ç°æˆåŠŸï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)