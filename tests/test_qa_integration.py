#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QAç³»ç»Ÿé›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•è®¯é£æ˜Ÿç«APIé›†æˆå’Œå®Œæ•´å¯¹è¯æµç¨‹
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.qa import ConversationChain, Memory, PromptTemplate
from modules.retrieval.vector_store import VectorStore
from modules.retrieval.bm25_retriever import BM25Retriever
from modules.retrieval.hybrid_retriever import HybridRetriever
from config.settings import settings


def test_llm_integration():
    """æµ‹è¯•LLMé›†æˆ"""
    print("=" * 50)
    print("æµ‹è¯•LLMé›†æˆ")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¯¹è¯é“¾
        conversation_chain = ConversationChain()
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        test_query = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"
        print(f"ç”¨æˆ·é—®é¢˜: {test_query}")
        
        # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
        mock_retrieved_docs = [
            {
                'text': 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è§†é¢‘å†…å®¹ï¼Œä»‹ç»äº†AIè§†é¢‘åŠ©æ‰‹çš„åŠŸèƒ½ã€‚',
                'start': 0.0,
                'end': 5.0,
                'similarity': 0.9
            }
        ]
        
        # æ‰‹åŠ¨è®¾ç½®æ£€ç´¢ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰
        conversation_chain.retriever = None  # æš‚æ—¶ä¸ä½¿ç”¨æ£€ç´¢å™¨
        
        # ç›´æ¥æµ‹è¯•LLMè°ƒç”¨
        response = conversation_chain._call_openai(test_query, "")
        print(f"AIå›ç­”: {response}")
        
        print("\nâœ… LLMé›†æˆæµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ LLMé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_conversation_memory():
    """æµ‹è¯•å¯¹è¯è®°å¿†åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å¯¹è¯è®°å¿†åŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¯¹è¯é“¾
        conversation_chain = ConversationChain()
        
        # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
        questions = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "äººå·¥æ™ºèƒ½æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "åˆšæ‰æåˆ°çš„åº”ç”¨ä¸­ï¼Œå“ªä¸ªæœ€é‡è¦ï¼Ÿ"
        ]
        
        for i, question in enumerate(questions):
            print(f"\nç¬¬{i+1}è½®å¯¹è¯:")
            print(f"ç”¨æˆ·: {question}")
            
            # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
            context = f"è¿™æ˜¯ç¬¬{i+1}ä¸ªé—®é¢˜çš„ç›¸å…³è§†é¢‘å†…å®¹ã€‚"
            
            # ç”Ÿæˆå›ç­”
            result = conversation_chain.chat(question, top_k=3)
            print(f"AI: {result['response']}")
            
            # æ˜¾ç¤ºå¯¹è¯å†å²
            print(f"å¯¹è¯è½®æ•°: {len(conversation_chain.conversation_history)}")
        
        print("\nâœ… å¯¹è¯è®°å¿†åŠŸèƒ½æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å¯¹è¯è®°å¿†åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_context_management():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¯¹è¯é“¾
        conversation_chain = ConversationChain()
        
        # åˆ›å»ºé•¿æ–‡æœ¬ä¸Šä¸‹æ–‡
        long_context = """
        è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„è§†é¢‘å†…å®¹ï¼Œç”¨äºæµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†åŠŸèƒ½ã€‚
        è§†é¢‘å†…å®¹åŒ…å«äº†å¾ˆå¤šä¿¡æ¯ï¼Œæ¯”å¦‚äººå·¥æ™ºèƒ½çš„å®šä¹‰ã€å†å²ã€åº”ç”¨ç­‰ã€‚
        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£å’Œæ„å»ºæ™ºèƒ½ä½“ã€‚
        è¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿæ„ŸçŸ¥ç¯å¢ƒå¹¶é‡‡å–è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–å…¶æˆåŠŸçš„æœºä¼šã€‚
        AIçš„å‘å±•å†å²å¯ä»¥è¿½æº¯åˆ°1950å¹´ä»£ï¼Œç»å†äº†å¤šæ¬¡å‘å±•æµªæ½®ã€‚
        å½“å‰ï¼ŒAIå·²ç»åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€é‡‘èé£æ§ç­‰ã€‚
        æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ã€‚
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚
        è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„å¦ä¸€ä¸ªé‡è¦é¢†åŸŸï¼Œè‡´åŠ›äºè®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
        è®¡ç®—æœºè§†è§‰åˆ™ä¸“æ³¨äºè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒå’Œè§†é¢‘ã€‚
        å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æ–¹æ³•ã€‚
        ä¸“å®¶ç³»ç»Ÿæ˜¯æ—©æœŸAIçš„ä¸€ç§å½¢å¼ï¼Œä½¿ç”¨è§„åˆ™å’ŒçŸ¥è¯†åº“æ¥è§£å†³é—®é¢˜ã€‚
        éšç€æŠ€æœ¯çš„å‘å±•ï¼ŒAIæ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½å’Œæ™®åŠã€‚
        æœªæ¥ï¼ŒAIå¯èƒ½ä¼šåœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚
        """
        
        # æµ‹è¯•æ¶ˆæ¯æ„å»º
        test_query = "è¯·æ€»ç»“è¿™ä¸ªè§†é¢‘çš„ä¸»è¦å†…å®¹ã€‚"
        messages = conversation_chain._build_messages(test_query, long_context)
        
        print(f"æ„å»ºçš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        print(f"ç³»ç»Ÿæç¤ºé•¿åº¦: {len(messages[0]['content'])}")
        
        if len(messages) > 1:
            print(f"è§†é¢‘å†…å®¹é•¿åº¦: {len(messages[1]['content'])}")
        
        # æµ‹è¯•tokenç®¡ç†
        managed_messages = conversation_chain._manage_token_limit(messages)
        print(f"ç®¡ç†åçš„æ¶ˆæ¯æ•°é‡: {len(managed_messages)}")
        
        print("\nâœ… ä¸Šä¸‹æ–‡ç®¡ç†æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ ä¸Šä¸‹æ–‡ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_complete_flow():
    """æµ‹è¯•å®Œæ•´æµç¨‹"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæ•´QAæµç¨‹")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_docs = [
            {
                'text': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£å’Œæ„å»ºæ™ºèƒ½ä½“ã€‚',
                'start': 0.0,
                'end': 5.0,
                'similarity': 0.9
            },
            {
                'text': 'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ã€‚',
                'start': 5.0,
                'end': 10.0,
                'similarity': 0.8
            },
            {
                'text': 'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘ã€‚',
                'start': 10.0,
                'end': 15.0,
                'similarity': 0.7
            }
        ]
        
        # åˆ›å»ºæ£€ç´¢å™¨ï¼ˆæ¨¡æ‹Ÿï¼‰
        class MockRetriever:
            def search(self, query, top_k=5):
                return test_docs[:top_k]
        
        # åˆ›å»ºå¯¹è¯é“¾
        conversation_chain = ConversationChain(retriever=MockRetriever())
        
        # æµ‹è¯•é—®ç­”
        test_query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        print(f"ç”¨æˆ·é—®é¢˜: {test_query}")
        
        result = conversation_chain.chat(test_query, top_k=3)
        print(f"AIå›ç­”: {result['response']}")
        print(f"æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(result['retrieved_docs'])}")
        
        print("\nâœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹QAç³»ç»Ÿé›†æˆæµ‹è¯•...")
    print("=" * 50)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("é…ç½®ä¿¡æ¯:")
    print(f"LLMæä¾›å•†: {settings.get_model_config('llm', 'provider')}")
    print(f"æ¨¡å‹åç§°: {settings.get_model_config('llm', 'openai', 'model_name')}")
    print(f"APIåœ°å€: {settings.get_model_config('llm', 'openai', 'base_url')}")
    print(f"æœ€å¤§token: {settings.get_model_config('llm', 'openai', 'max_tokens')}")
    print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {settings.get_model_config('qa_system', 'max_context_length')}")
    print(f"å¯¹è¯å†å²é•¿åº¦: {settings.get_model_config('qa_system', 'history_length')}")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_llm_integration,
        test_conversation_memory,
        test_context_management,
        test_complete_flow
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼QAç³»ç»Ÿé›†æˆæˆåŠŸï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œå®ç°ã€‚")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)